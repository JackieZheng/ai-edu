Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 第9章 单入单出的的双层神经网络

## 9.0 非线性回归

### 9.0.1 提出问题一

我们在第5章学习了线性回归的解决方案，但是在工程实践中，我们最常遇到不是线性问题，而是非线性问题。例如下面这条正弦曲线：

<img src="../Images/9/sin_data.png">

|样本|x|y|
|---|---|---|
|1|0.1199|0.6108|
|2|0.0535|0.3832|
|3|0.6978|0.9496|
|...|...|...|

问题：使用神经网络如何拟合一条曲线？

### 9.0.2 多项式回归

多项式回归有几种形式：

#### 一元一次多项式

单变量的线性回归，我们在第4章学习过相关内容。其模型为：

$$z = x w + b \tag{1}$$

#### 多元一次多项式

多变量的线性回归，我们在第5章学习过相关内容。其模型为：

$$z = x_1 w_1 + x_2 w_2 + ...+ x_m w_m + b \tag{2}$$

#### 一元多次多项式

单变量的非线性回归，比如上面这个正弦曲线的拟合问题，很明显不是线性问题，但是只有一个x特征值，所以不满足前两种形式。如何解决这种问题呢？

有一个定理：任意一个函数在一个较小的范围内，都可以用多项式任意逼近，因此在实际工程实践中，有时候可以不管y值与x值的数学关系究竟是什么，而是强行用回归分析方法进行近似的拟合。

对于只有一个特征值的问题，人们发明了一种聪明的办法，就是把特征值的高次方作为另外的特征值，加入到回归分析中，用公式描述：

$$z = x w_1 + x^2 w_2 + ... + x^m w_m + b \tag{3}$$

换一种形式，令：$x_1 = x，x_2=x^2，...，x_m=x^m$，则：

$$z = x_1 w_1 + x_2 w_2 + ... + x_m w_m + b \tag{4}$$

可以看到公式4和上面的公式2是一样的。

#### 多元多次多项式

多变量的非线性回归，其参数与特征组合繁复，但最终都可以归结为公式2和公式4的形式。

所以，不管是几元几次多项式，我们都可以使用第5章学到的方法来解决，在下一个小节中，会用代码具体实现。

### 9.0.3 提出问题二

前面的正弦函数，看上去是非常有规律的，也许单层神经网络很容易就做到了。如果是更复杂的曲线，单层神经网络还能轻易做到吗？比如下面这张图，给出如下一批训练数据，如何使用神经网络方法来拟合这条曲线？

<img src="..\Images\9\Sample.png">

|样本|x|y|
|---|---|---|
|1|0.606|-0.113|
|2|0.129|-0.269|
|3|0.582|0.027|
|...|...|...|
|1000|0.199|-0.281|

原则上说，如果你有足够的耐心，愿意花很高的时间成本和计算资源，总可以用多项式回归的方式来解决这个问题，但是，在本章，我们将会学习另外一个定理：前馈神经网络的通用近似定理。

上面这条“蛇形”曲线，实际上是由下面这个公式添加噪音后生成的：

$$y=0.4x^2 + 0.3xsin(15x) + 0.01cos(50x)-0.3$$

我们特意把数据限制在[0,1]之间，避免做归一化的麻烦。要是觉得这个公式还不够复杂，大家可以用更复杂的公式去自己做试验。

以上问题可以叫做非线性回归，即自变量X和因变量Y之间不是线性关系。常用的传统的处理方法有线性迭代法、分段回归法、迭代最小二乘法等。在神经网络中，解决这类问题的思路非常简单，就是使用带有一个隐层的两层神经网络。

### 9.0.4 通用近似定理

这里有一篇论文，Kurt Hornik在1991年发表的，说明了含有一个隐层的神经网络能拟合任意复杂函数：

https://www.sciencedirect.com/science/article/pii/089360809190009T

Abstract - We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to L(u) performance criteria, for arbitrary finite input envrionment measres u, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives.

简言之：两层前馈神经网络（即一个隐层加一个输出层）和至少一层具有任何一种挤压性质的激活函数，只要隐层的神经元的数量足够，它能以任意精度来近似拟合任意连续函数。原文提到Borel可测函数，超出了本课程的范围。

#### 直观解释

还有一种直观的解释是这样的（个人认为这不是神经网络的工作原理），用无数个矩形的拼接来近似一条曲线：

|粗粒度|细粒度|
|---|---|
|<img src="..\Images\9\histogram1.jpg">|<img src="..\Images\9\histogram2.jpg">|

它的理论基础是：

<img src="..\Images\9\histogram3.jpg">

假设在隐层有两个神经元，都配置有Sigmoid激活函数。第一个神经元在-0.01处产生一个阶跃，第二个神经元在+0.01处产生一个阶跃，都是用Sigmoid函数完成的，然后用第一个神经元的输出减去第二个神经元的输出（设置权重值为[1,-1]）,就会形成一个“门”。无数个这样的“门”就能模拟出一条曲线。

对于三维空间，它是这个样子的：

|阶跃面|封闭塔|
|---|---|
|<img src="../Images/9/histogram4.jpg">|<img src="../Images/9/histogram5.jpg">|

在三维空间中，两个有不同偏置值的sigmoid激活函数相减，我们将得到左侧的等效曲线。如果我们采用另一个水平垂直的塔架到现在组合的曲线上。在叠加这两个水平垂直的开放式塔时，我们就可以得到封闭的塔。然后就可以用封闭塔模拟任何三维曲面。

### 参考资料

- https://towardsdatascience.com/representation-power-of-neural-networks-8e99a383586
